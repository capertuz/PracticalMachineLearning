---
output:
  html_document: default
  pdf_document: default
---
---
title: "Where is the beach? Creating a model to exercise your biceps in the best way"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(car)
library(GGally)
library(caret)
library(randomForest)
library(rpart)
library(rattle)
setwd("C:/Coursera/Data Science/8 Practical Machine Learning Project")


```
## Introduction

The exponential availability of wearable devices allows us to capture vast amounts of data. More and more companies use the data to find better insights about our lifestyles. Devices like Fitbit, and its companion app, can determine how well you slept last night and what type of exercise you are performing. In this document we want to explore how the data can help us to determine not only the *what* but also *how* well you are exercising, specifically how well you are doing your bicep curls.


## Feature Selection

After exploring the data, we can see that a lot of variables are calculated eg.  avg_pitch_dumbbell    stddev_pitch_dumbbell, etc. Coincidentally, in the testing data set most of the calculated variables don't contain any data and it wouldn't make any sense to include them in the model, otherwise we wouldn't be able to get the prediction. On the other hand, dropping the calculated variables decreases the variables dependencies. In Appendix 2 we can see how there is a linear relationship between the total acceleration in the belt and its component in the Z axis.

Other variables, such as timestamps, observation number, and username were not included in the model because they are not relevant. For example, the time you go to the gym is not important to determine if your bicep curl is correct or incorrect.

## Crosss-validation

For cross-validation, random sampling is used via the function createDataPartition (see Appendix 4), in this way we can test the accuracy of our model using only the testing data. To partition the data the 60-40 rule of thumb for medium sample size is used, i.e 60% of data for training 40% for validating.

## Model Selection

Given that we need to classify the data into one of five classes of exercises, one correct and four incorrect, we will compare two classification models: a tree and a random forest.

Appendix 8 shows the classification tree generated by the tree model. 

Random forests are characterized for their accuracy and their loss of interpretability. After running the confusion matrix for both models, we can see in Appendix 5 that the random forest accuracy is higher and therefore this will be the model selected for prediction.


## Expected Out of Sample error

To avoid overfitting, we measured the accuracy of our models in the validating data set. The out of sample error is obtained by subtracting the accuracy from 1. 

In Appendix 6, the predicted vs actual values tables show how the error is distributed on each model. This also confirms that the random forest is more accurate than the tree model

## Prediction

The random forest model is used on the testing data set to get the predictions (see Appendix 7).



## Appendix


### Appendix 1: Data processing
```{r process, echo=TRUE}
trainingraw = read.csv("pml-training.csv")
testingraw = read.csv("pml-testing.csv")

#define function to get columns with no data 

na.test <-  function (x) {
  w <- sapply(x, function(x)all(is.na(x)))
  return(which(w))
}
natest <- na.test (testingraw)

#drop columns with no data on the testing environment, otherwise it is impossible to run any model created
trainingdata <- trainingraw[ , !(names(trainingraw) %in% names(natest))]

#drop columns irrelevant to the analysis
trainingdata <- trainingdata[ , !(names(trainingdata) %in% c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window"))]
```
### Appendix 2: Plot Total
```{r plotTotal, echo=TRUE , fig.width=12,fig.height=12}
featurePlot(x = trainingdata[, c("accel_belt_x","accel_belt_y","accel_belt_z","total_accel_belt", "classe")], y = trainingdata$classe, plot = "pairs")
```
### Appendix 3: Data processing 2
```{r process2, echo=TRUE}

#drop columns that contain "total" as there is a relationship with the x,y and z components
totals <- names(trainingdata %>% select(contains("total")))
trainingdata <- trainingdata[ , !(names(trainingdata) %in% totals)]

#drop incomplete observations in the training dataset
trainingdata <- trainingdata[complete.cases(trainingdata), ]

#sync the testing data
testing <- testingraw[ , (names(testingraw) %in% names(trainingdata))]

#drop incomplete observations in the testing dataset
testing <- testing[complete.cases(testing), ]
```
### Appendix 4: Cross Validation
```{r cross, echo=TRUE}
#set seed
set.seed(34765)

#Partition the training data into training and validating
inTrain = createDataPartition(trainingdata$classe, p=0.6, list=FALSE)
training = trainingdata[inTrain,]
validating = trainingdata[-inTrain,]

```
### Appendix 5: Model Selection
```{r model, echo=TRUE}
#Fit tree model, method = "class" is assume given that the variable classe is a factor
modeltreeFit <- rpart(classe ~ .,data=training)

#predict using the validation data
predicttree <- predict(modeltreeFit,newdata=validating, type = "class")

#get accuracy
accuracytree <- confusionMatrix(predicttree, validating$classe)$overall['Accuracy']
accuracytree

#Fit random forest model
modelrfFit <- randomForest(classe ~ .,data=training)

#predict using the validation data
predictrf <- predict(modelrfFit,newdata=validating)

#get accuracy
accuracyrf <- confusionMatrix(predictrf, validating$classe)$overall['Accuracy']
accuracyrf
```
### Appendix 6: Out of Sample Error
```{r error, echo=TRUE}
#display out of sample errors

errortree <- 1- accuracytree
errortree
errorrf <- 1- accuracyrf
errorrf

#display predicted values vs actual on the validating set

table(predicttree, validating$classe)
table(predictrf, validating$classe)
```
### Appendix 7: Predictions
```{r predictions, echo=TRUE}
#Predictions

predictions <- predict(modelrfFit,newdata=testing)
predictions

```
### Appendix 8: Classification Tree
```{r tree, echo=TRUE, fig.width=12,fig.height=12}
#Tree

fancyRpartPlot(modeltreeFit, sub="")

```